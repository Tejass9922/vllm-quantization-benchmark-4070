{"prompt": "You are a helpful assistant. Explain what quantization is in LLM inference in 5 bullet points."}
{"prompt": "Summarize the following text in 2 sentences: Large language models can be served efficiently with optimized attention kernels and batching."}
{"prompt": "Write a Python function to compute moving average over a list of floats."}
{"prompt": "Given a short customer support transcript, extract the customer intent and the key entities."}
{"prompt": "Explain the difference between latency p95 and throughput in an inference service."}
